{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "import numpy as np\nimport time",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "# T1: dichotomy\ndef bisection(f, a, b, epsilon = 1e-4, max_iter = 100):  \n    if a >= b:\n        raise ValueError(\"b must be bigger than a\")\n        \n    if f(a) * f(b) >= 0:  \n        raise ValueError(\"f(a) * f(b) must be negative\")  \n  \n    fa, fb = f(a), f(b)\n    \n    # if for iter_cnt in range(max_iter), although it may not report error out of for, it's not a good way\n    iter_cnt = 0\n    for _ in range(max_iter): \n        x = (a + b) / 2\n        fx = f(x)\n        error = np.abs(fx)\n        if error < epsilon:\n            break  \n        \n        if fa * fx < 0:\n            b = x  \n            fb = fx  \n        else: \n            a = x  \n            fa = fx\n\n        iter_cnt += 1\n\n    return x, error, iter_cnt\n\n\ndef f(x):\n    return x**4/4 - 4*x**3/3 + 5*x**2/2 - 2*x\n\ndef df(x):\n    return x**3 - 4*x**2 + 5*x - 2\n\nroot, _, _ = bisection(df, 0, 4)\nprint(f\"{root}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "2.0\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": "# T2: 0.618\ndef f(x):\n    return np.exp(-x) + x**2\n\ndef goldencut(f, a, b, epsilon = 1e-4, max_iter = 100):\n    if a >= b:\n        raise ValueError(\"b must be bigger than a\")\n\n    t = (np.sqrt(5) - 1)/2\n    iter_cnt = 0\n    for _ in range(max_iter):\n        error = b - a\n        if error < epsilon:\n            break\n        \n        x1 = a + (b - a)*(1 - t)\n        x2 = a + (b - a)*t\n        if f(x1) < f(x2):\n            b = x2\n        else:\n            a = x1\n        iter_cnt += 1\n    return (a + b)/2, error, iter_cnt\n\nminpoint, _, _ = goldencut(f, -2, 3)\nprint(f\"{minpoint}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "0.35173474094718815\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": "# T3: Newton\ndef f(x):\n    return x**4 - 4*x**3 - 6*x**2 - 16*x + 4\n    \ndef df(x):\n    return 4*x**3 - 12*x**2 - 12*x - 16\n\ndef ddf(x):\n    return 12*x**2 - 24*x - 12\n\ndef newton(f, x, df, epsilon = 1e-4, max_iter = 100):\n    for iter_cnt in range(max_iter):\n        fx = f(x)\n        error = np.abs(fx)\n        if error < epsilon:\n            break\n        \n        x = x - fx/df(x)\n\n    return x, error, iter_cnt\n\nroot, _, _ = newton(df, 2.5, ddf)\nprint(f\"{root}\")\nroot, _, _ = newton(df, 3, ddf)\nprint(f\"{root}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "4.000000000017841\n4.000000184090883\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": "# T4: steepest descent, conjugate gradient, DFP\nclass OptimizationAlgorithm: \n    pass\n\ndef StepSelect_Newton(x, grad_f, Hesse_f, p, step, epsilon = 1e-7, max_iter = 5):\n    for _ in range(max_iter):\n        fstep = np.dot(grad_f(x + step*p), p)\n        error = np.abs(fstep)\n        if error < epsilon:\n            break\n        \n        # dlambda f(x + lambda*p) = grad_f(x + lambda*p)*p\n        step = step - fstep/np.dot(Hesse_f(x + step*p)@p, p)\n        \n    return step, x + step*p\n\ndef StepSelect_GoldenCut(f, a, b, epsilon = 1e-7, max_iter = 100):\n    if a >= b:\n        raise ValueError(\"b must be bigger than a\")\n\n    t = (np.sqrt(5) - 1)/2\n    for _ in range(max_iter):\n        error = b - a\n        if error < epsilon:\n            break\n        \n        x1 = a + (b - a)*(1 - t)\n        x2 = a + (b - a)*t\n        if f(x1) < f(x2):\n            b = x2\n        else:\n            a = x1\n    return (a + b)/2\n\n# Steepest Descent\ndef SteepestDescent(f, grad_f, x, StepSelectMethod, Hesse_f, init_step = 0.1, epsilon = 1e-7, max_iter = 100):\n    iter_cnt = 0\n    for _ in range(max_iter):\n        g = grad_f(x)\n        p = -g\n        if np.linalg.norm(g) < epsilon:\n            break\n\n        # find step = argmin_step f(x + step*p)\n        if StepSelectMethod == \"fixed\":\n            step = init_step\n        elif StepSelectMethod == \"Newton\":\n            step, _ = StepSelect_Newton(x, grad_f, Hesse_f, p, 0) \n        x = x + step*p\n        iter_cnt += 1\n    \n    return x, f(x), iter_cnt\n\n\n# Conjugate Gradient (FR)\nclass ConjugateGradient_FR():\n    def __init__(self, x, f, epsilon = 1e-7, max_iter = 100):\n        self.x = x\n        self.f = f\n        self.epsilon = epsilon\n        self.max_iter = max_iter\n        self.step_select_methods = {  \n            \"fixed\": {\"init_step\": 0.1},  \n            \"Newton\": {\"Hesse_f\": None}, \n        }\n\n    def optimize(self, grad_f, StepSelectMethod, **kwargs):\n        if StepSelectMethod not in self.step_select_methods:  \n            raise ValueError(f\"Unknown step select method: {StepSelectMethod}\")\n        x = self.x\n        f = self.f\n        epsilon = self.epsilon\n        max_iter = self.max_iter\n        n = np.size(x)\n\n        g = grad_f(x)\n        iter_cnt = 0\n        if np.linalg.norm(g) >= epsilon:\n            if StepSelectMethod == \"fixed\":\n                pass\n            elif StepSelectMethod == \"Newton\":\n                Hesse_f = kwargs[\"Hesse_f\"]\n            \n            end = False\n            for _ in range(max_iter):\n                p = -g\n                iter_in = 0\n                while(1):\n                    # find step = argmin_step f(x + step*p)\n                    if StepSelectMethod == \"fixed\":\n                        step = self.step_select_methods[\"fixed\"][\"init_step\"]\n                        x = x + step*p\n                    elif StepSelectMethod == \"Newton\":\n                        # dlambda f(x + lambda*p) = dot(grad_f(x + lambda*p), p)\n                        # ddlambda f(x + lambda*p) = dot(Hesse_f(x + lambda*p)@p, p)\n                        _, x = StepSelect_Newton(x, grad_f, Hesse_f, p, 0)\n        \n                    g_pre = g\n                    g = grad_f(x)\n        \n                    if np.linalg.norm(g) < epsilon:\n                        end = True\n                        break\n    \n                    iter_in += 1\n                    if iter_in == n:\n                        break\n                        \n                    p = -g + (np.linalg.norm(g)/np.linalg.norm(g_pre))**2*p\n                    \n                if end == True:\n                    break\n                iter_cnt += 1\n        return x, f(x), iter_cnt\n\n# DFP\nclass DFP():\n    def __init__(self, x, f, epsilon = 1e-6, max_iter = 100):\n        self.x = x\n        self.f = f\n        self.epsilon = epsilon\n        self.max_iter = max_iter\n        self.step_select_methods = {  \n            \"fixed\": {\"init_step\": 0.1},  \n            \"Newton\": {\"Hesse_f\": None},\n            \"GoldenCut\": {\"max_step\": 1},\n        }\n\n    def optimize(self, grad_f, StepSelectMethod, **kwargs):\n        if StepSelectMethod not in self.step_select_methods:  \n            raise ValueError(f\"Unknown step select method: {StepSelectMethod}\")\n        x = self.x\n        f = self.f\n        epsilon = self.epsilon\n        max_iter = self.max_iter\n        n = np.size(x)\n\n        g = grad_f(x)\n        iter_cnt = 0\n        if np.linalg.norm(g) >= epsilon:\n            if StepSelectMethod == \"fixed\":\n                pass\n            elif StepSelectMethod == \"Newton\":\n                Hesse_f = kwargs[\"Hesse_f\"]\n            elif StepSelectMethod == \"GoldenCut\":\n                if \"max_step\" in kwargs:\n                    max_step = kwargs[\"max_step\"]\n                else:\n                    max_step = self.step_select_methods[\"GoldenCut\"][\"max_step\"]\n            \n            end = False\n            for _ in range(max_iter):\n                H = np.eye(n)\n                p = -H@g\n                iter_in = 0\n                while(1):\n                    # find step = argmin_step f(x + step*p)\n                    if StepSelectMethod == \"fixed\":\n                        step = self.step_select_methods[\"fixed\"][\"init_step\"]\n                        x = x + step*p\n                    elif StepSelectMethod == \"Newton\":\n                        # dlambda f(x + lambda*p) = dot(grad_f(x + lambda*p), p)\n                        # ddlambda f(x + lambda*p) = dot(Hesse_f(x + lambda*p)@p, p)\n                        step, x = StepSelect_Newton(x, grad_f, Hesse_f, p, 0)\n                    elif StepSelectMethod == \"GoldenCut\":\n                        def f_x_plus_lp(l):\n                            return f(x + l*p)\n                        step = StepSelect_GoldenCut(f_x_plus_lp, 0, max_step)\n                        x = x + step*p\n        \n                    g_pre = g\n                    g = grad_f(x)\n                    \n                    if np.linalg.norm(g) < epsilon:\n                        end = True\n                        break\n    \n                    iter_in += 1\n                    if iter_in == n:\n                        break\n                        \n                    delta_x = step*p\n                    delta_g = g - g_pre\n                    r = H@delta_g\n                    # np.outer(x, x) is just delta_x[:, np.newaxis] @ delta_x[np.newaxis, :]\n                    H += np.outer(delta_x, delta_x)/np.dot(delta_x, delta_g) - np.outer(r, r)/np.dot(delta_g, r)\n                    \n                if end == True:\n                    break\n\n                iter_cnt += 1\n        return x, f(x), iter_cnt\n\n\ndef f(x):\n    return 4*x[0]**2 + 4*x[1]**2 - 4*x[0]*x[1] - 12*x[1]\n\ndef grad_f(x):\n    return np.array([8*x[0] - 4*x[1], 8*x[1] - 4*x[0] - 12])\n\ndef Hesse_f(x):\n    return np.array([[8, -4], [-4, 8]])\n\n# Attention: in python, vector and matrix is different.\n# When use vector (such as variable x), it's better to use vector, x = np.array([1, 1])\n# In matrix calculation, if it will not be confused, numpy will decide whether it is row or column vector automatically,\n# otherwise, make a special announcement as `x[:, np.newaxis]`\n\nstart = time.time()\nminpoint, fmin, itertime = SteepestDescent(f, grad_f, np.array([-1/2, 1]), \"fixed\", Hesse_f)\nend = time.time()\nprint(f\"Steepest Descent, Step select method: fixed\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\nstart = time.time()\nminpoint, fmin, itertime = SteepestDescent(f, grad_f, np.array([-1/2, 1]), \"Newton\", Hesse_f)\nend = time.time()\nprint(f\"Steepest Descent, Step select method: Newton\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\n\n\noptimizer = ConjugateGradient_FR(np.array([-1/2, 1]), f)\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, \"Newton\", Hesse_f = Hesse_f)\nend = time.time()\nprint(f\"Conjugate Gradient (FR), Step select method: Newton\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\n\n\noptimizer = DFP(np.array([-1/2, 1]), f)\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, \"GoldenCut\")\nend = time.time()\nprint(f\"DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, \"Newton\", Hesse_f = Hesse_f)\nend = time.time()\nprint(f\"DFP, Step select method: Newton\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Steepest Descent, Step select method: fixed\nmin point: [0.99999999 1.99999999], fmin: -12.0, iter time: 36\nrun time: 0.0009999275207519531\n\nSteepest Descent, Step select method: Newton\nmin point: [0.99997635 1.99998423], fmin: -11.999999998260051, iter time: 100\nrun time: 0.0039997100830078125\n\nConjugate Gradient (FR), Step select method: Newton\nmin point: [1. 2.], fmin: -12.0, iter time: 0\nrun time: 0.0009999275207519531\n\nDFP, Step select method: GoldenCut\nmin point: [0.99999996 1.99999989], fmin: -11.999999999999964, iter time: 20\nrun time: 0.017999887466430664\n\nDFP, Step select method: Newton\nmin point: [0.99997635 1.99998423], fmin: -11.999999998260051, iter time: 100\nrun time: 0.0070002079010009766\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 30
    },
    {
      "cell_type": "code",
      "source": "# T5: Exterior & Interior Point\nclass ConstraintOptimization:\n    pass\n\n\nclass ExteriorPoint:\n    # Generally, accuracy of outer process should be lower then inner one, otherwise it can't converge\n    def __init__(self, x, f, g, h, init_M = 1, c = 4, epsilon = 1e-3, max_iter = 100):\n        self.x = x\n        self.f = f\n        self.g = g\n        self.h = h\n        self.init_M = init_M\n        self.c = c\n        self.epsilon = epsilon\n        self.max_iter = max_iter\n\n    def p(self, x, g, h):\n        sum_g = 0\n        sum_h = 0\n        if g != None:\n            gs = g(x)\n            sum_g = np.sum(np.minimum(gs, np.zeros(np.size(gs)))**2)\n        if h != None:\n            sum_h = np.sum(h(x)**2)\n        \n        return sum_g + sum_h\n\n    def grad_p(self, x, g, h, grad_g, grad_h):\n        # grad_p = sum(g(x)*grad_g - |g(x)|*grad_g) + sum(2*h(x)*grad_h))\n        #        = sum(2*min{g(x), 0}*grad_g) + sum(2*h(x)*grad_h))\n        n = np.size(x)\n        sum_g = np.zeros(n)\n        sum_h = np.zeros(n)\n        if g != None:\n            gx = g(x)\n            grad_gx = grad_g(x)\n            # here use g(x) is a single-row matrix, we want each column of g(x) multiplies relative column of grad_g(x)\n            sum_g = np.sum(gx*grad_gx - np.abs(gx)*grad_gx, axis = 1)\n        if h != None:\n            hx = h(x)\n            grad_hx = grad_h(x)\n            sum_h = np.sum(2*hx*grad_hx, axis = 1)\n        \n        return sum_g + sum_h\n    \n    def optimize(self, grad_f, grad_g, grad_h):\n        x = self.x\n        f = self.f\n        g = self.g\n        h = self.h \n        M = self.init_M\n        c = self.c\n        epsilon = self.epsilon\n        max_iter = self.max_iter\n\n        iter_cnt = 0\n        for _ in range(max_iter):\n            def f_plus_Mp(x):\n                return f(x) + M*self.p(x, g, h)\n\n            def grad_f_plus_Mp(x):\n                return grad_f(x) + M*self.grad_p(x, g, h, grad_g, grad_h)\n            \n            optimizer = DFP(x, f_plus_Mp)\n            x, f_plus_Mp_min, _ = optimizer.optimize(grad_f_plus_Mp, \"GoldenCut\", max_step = 0.1)\n            # must judge error after once optimized, because if init point is in the field, error = 0\n            error = np.abs(f_plus_Mp_min - f(x))\n            if error < epsilon:\n                break\n            \n            M = c*M\n            iter_cnt += 1  \n        return x, f(x), iter_cnt\n\nclass InteriorPoint:\n    # accuracy should not be too high, otherwise the point will go out of the boundary\n    def __init__(self, x, f, g, init_r = 1, c = 0.1, epsilon = 0.05, max_iter = 100):\n        self.x = x\n        self.f = f\n        self.g = g\n        self.init_r = init_r\n        self.c = c\n        self.epsilon = epsilon\n        self.max_iter = max_iter\n        self.punish_functions = {\n            \"reciprocal\",\n            \"logarithm\",\n        }\n\n    def B_r(self, x, g):\n        sum_g = 0\n        if g != None:\n            sum_g = np.sum(1/g(x))\n        \n        return sum_g\n\n    def grad_B_r(self, x, g, grad_g):\n        # grad_B_r = sum(-grad_g(x)/[g(x)]^2)\n        sum_g = np.zeros(np.size(x))\n        if g != None:\n            sum_g = np.sum(-1/g(x)**2*grad_g(x), axis = 1)\n        \n        return sum_g\n\n    def B_l(self, x, g):\n        sum_g = 0\n        if g != None:\n            sum_g = -np.sum(np.log(g(x)))\n        return sum_g\n\n    def grad_B_l(self, x, g, grad_g):\n        # grad_B_l = -sum(grad_g(x)/g(x))\n        sum_g = np.zeros(np.size(x))\n        if g != None:\n            sum_g = -np.sum(1/g(x)*grad_g(x), axis = 1)\n        \n        return sum_g\n    \n    def optimize(self, grad_f, grad_g, PunishFunction):\n        x = self.x\n        f = self.f\n        g = self.g \n        r = self.init_r\n        c = self.c\n        epsilon = self.epsilon\n        max_iter = self.max_iter\n\n        if PunishFunction not in self.punish_functions:\n            raise ValueError(f\"Unknown punish function: {PunishFunction}\")\n        if PunishFunction == \"reciprocal\":\n            B = self.B_r\n            grad_B = self.grad_B_r\n        elif PunishFunction == \"logarithm\":\n            B = self.B_l\n            grad_B = self.grad_B_l\n        \n        iter_cnt = 0\n        for _ in range(max_iter):\n            def f_plus_rB(x):\n                return f(x) + r*B(x, g)\n\n            def grad_f_plus_rB(x):\n                return grad_f(x) + r*grad_B(x, g, grad_g)\n\n            # observe the convergence process\n            print(f\"x:{x} f(x):{f(x)} r*B:{r*B(x, g)}\")\n            \n            optimizer = DFP(x, f_plus_rB)\n            x, f_plus_rB_min, _ = optimizer.optimize(grad_f_plus_rB, \"GoldenCut\", max_step = 0.01)\n            # must judge error after once optimized, because if init point is in the field, error = 0\n            error = np.abs(f_plus_rB_min - f(x))\n            if error < epsilon:\n                break\n            \n            r = c*r\n            iter_cnt += 1\n        return x, f(x), iter_cnt",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 28
    },
    {
      "cell_type": "code",
      "source": "# In this case, as the number of constraints can be single or multiple, constraints should represents as single-row matrix\n\n# different dims of x by row, different constraints by column\ndef f(x):\n    return x[0]**2 + x[1]**2\n\ndef grad_f(x):\n    return np.array([2*x[0], 2*x[1]])\n\n# remember to add an extra [] even if g(x) has only one formula,\n# otherwise it will be regarded as vector but not matrix (eg. in numpy.sum, axis = 1 is not available)\ndef g(x):\n    return np.array([[x[0] - 1]])\n\ndef grad_g(x):\n    return np.array([[1], [0]])\n\nprint(f\"function 1\")\noptimizer = ExteriorPoint(np.array([2, 2]), f, g, None)\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, grad_g, None)\nend = time.time()\nprint(f\"Exterior point, non-constraint method: DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\noptimizer = InteriorPoint(np.array([2, 2]), f, g)\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, grad_g, \"reciprocal\")\nend = time.time()\nprint(f\"Interior point, punish function: reciprocal, non-constraint method: DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, grad_g, \"logarithm\")\nend = time.time()\nprint(f\"Interior point, punish function: logarithm, non-constraint method: DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "function 1\nExterior point, non-constraint method: DFP, Step select method: GoldenCut\nmin point: [9.99024390e-01 3.24611342e-07], fmin: 0.9980497319455207, iter time: 5\nrun time: 0.08800005912780762\n\nx:[2 2] f(x):8 r*B:10.0\nx:[ 2.43342767e+00 -3.45953032e-09] f(x):5.921570246498887 r*B:0.6976285011546657\nx:[ 1.56519771e+00 -5.22243203e-16] f(x):2.449843873161911 r*B:0.17692923755796758\nx:[5.61227777e-02 3.72204664e-17] f(x):0.003149766175120704 r*B:-0.010594598284168099\nInterior point, punish function: reciprocal, non-constraint method: DFP, Step select method: GoldenCut\nmin point: [ 5.05089635e-03 -4.13672368e-19], fmin: 2.5511553937870017e-05, iter time: 3\nrun time: 0.03299999237060547\n\nx:[2 2] f(x):8 r*B:-0.0\nx:[2.79128769e+00 2.49881952e-07] f(x):7.791286966323492 r*B:-0.5829347409090372\nx:[1.36602540e+00 1.17088329e-07] f(x):1.8660253974930239 r*B:0.10050525450338095\nx:[1.04772255e+00 1.05032587e-07] f(x):1.0977225396904087 r*B:0.030423512673469995\nInterior point, punish function: logarithm, non-constraint method: DFP, Step select method: GoldenCut\nmin point: [1.00497525e+00 1.15884482e-07], fmin: 1.0099752456727895, iter time: 3\nrun time: 0.0559999942779541\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 17
    },
    {
      "cell_type": "code",
      "source": "def f(x):\n    return -x[0]*x[1]\n\ndef grad_f(x):\n    return np.array([-x[1], -x[0]])\n\n# g is a single-row matrix, but not vector\ndef g(x):\n    return np.array([[-x[0] - x[1]**2 + 1, x[0] + x[1]]])\n\ndef grad_g(x):\n    return np.array([[-1, 1], [-2*x[1], 1]])\n\nprint(f\"function 2\")\noptimizer = ExteriorPoint(np.array([2, 2]), f, g, None)\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, grad_g, None)\nend = time.time()\nprint(f\"Exterior point, non-constraint method: DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\noptimizer = InteriorPoint(np.array([0.5, 0.5]), f, g)\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, grad_g, \"reciprocal\")\nend = time.time()\nprint(f\"Interior point, punish function: reciprocal, non-constraint method: DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")\n\nstart = time.time()\nminpoint, fmin, itertime = optimizer.optimize(grad_f, grad_g, \"logarithm\")\nend = time.time()\nprint(f\"Interior point, punish function: logarithm, non-constraint method: DFP, Step select method: GoldenCut\")\nprint(f\"min point: {minpoint}, fmin: {fmin}, iter time: {itertime}\")\nprint(f\"run time: {end - start}\\n\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "function 2\nExterior point, non-constraint method: DFP, Step select method: GoldenCut\nmin point: [0.66744612 0.57765251], fmin: -0.3855519289214287, iter time: 4\nrun time: 0.9879999160766602\n\nx:[0.5 0.5] f(x):-0.25 r*B:5.0\nx:[0.19722759 0.45421414] f(x):-0.08958355938882287 r*B:0.32116096611014505\nx:[0.38249179 0.4528998 ] f(x):-0.17323045618435354 r*B:0.03621932595352922\nx:[0.57234874 0.54021587] f(x):-0.3091918691753714 r*B:0.00826161419513253\nInterior point, punish function: reciprocal, non-constraint method: DFP, Step select method: GoldenCut\nmin point: [0.63732836 0.56621404], fmin: -0.36086426387137066, iter time: 3\nrun time: 0.6490001678466797\n\nx:[0.5 0.5] f(x):-0.25 r*B:1.3862943611198906\nx:[0.2608292  0.44877479] f(x):-0.11705356828713369 r*B:0.09633688469896051\nx:[0.5511195  0.53431868] f(x):-0.29447344301899026 r*B:0.017296679595515816\nInterior point, punish function: logarithm, non-constraint method: DFP, Step select method: GoldenCut\nmin point: [0.65332398 0.57393144], fmin: -0.37496317266498785, iter time: 2\nrun time: 0.4089999198913574\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 29
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}